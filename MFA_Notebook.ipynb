{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnloop/MFA-Universal-Notebook/blob/slicer/MFA_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/)**\n",
        "_Command line utility for forced alignment using Kaldi_\n",
        "\n",
        "\\\n",
        "____\n",
        "\\\n",
        "\n",
        "## _Known issues:_\n",
        "- It really struggles with long silences, long notes and humming.\n",
        "\n",
        "- It's really dependent on Whisper's performance as well, which isn't always perfect: if you want a better base it's highly recommended to edit the transcriptions!\n",
        "\n",
        "- The pretrained dictionaries for MFA are often lackluster and/or inaccurate when transposed to singing, which affects the label quality (I've particularly noticed this with French).\n",
        "\n",
        "- It's possible to supplement the dictionaries with G2P models, but I haven't implemented that.\n",
        "\n",
        "\\\n",
        "____\n"
      ],
      "metadata": {
        "id": "sfxFpUjTLMzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huge thanks to PixPrucer and HAI-D for basically making every part of this notebook. I just updated things around basically â›¹"
      ],
      "metadata": {
        "id": "M52QImcigVUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Upload files**"
      ],
      "metadata": {
        "id": "VdX5jLV9btep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jpHc_ficaXQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip and optionally slice corpus\n",
        "#@markdown Unzip your dataset for transcription stuff. Make sure it is an archive only containing wavs (15-30 seconds in length recommended).<br>If your samples are long you can use the audio slicer by setting slice_samples to True.\n",
        "\n",
        "file_location = '/content/drive/MyDrive/wav.zip' #@param {type:\"string\"}\n",
        "slice_samples = True #@param {type:\"boolean\"}\n",
        "\n",
        "!7z x \"$file_location\" -o/content/db\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Wavs extracted in db folder\")\n",
        "\n",
        "if slice_samples:\n",
        "  !git clone https://github.com/openvpi/audio-slicer\n",
        "  !pip install librosa\n",
        "  !pip install soundfile\n",
        "  !mkdir /content/slicer-temp\n",
        "  !mv /content/db/*.wav /content/slicer-temp/\n",
        "  import os\n",
        "\n",
        "  # Change directory to the audio-slicer folder\n",
        "  os.chdir('/content/slicer-temp')\n",
        "\n",
        "  # Loop through all files in the folder\n",
        "  for filename in os.listdir():\n",
        "      # Check if the file is a .wav file\n",
        "      if filename.endswith('.wav'):\n",
        "          # Execute the command for the current file\n",
        "          !python /content/audio-slicer/slicer2.py /content/slicer-temp/\"$filename\" --out /content/db/\n",
        "  from IPython.display import clear_output\n",
        "  clear_output()\n",
        "  print(\"Your sliced audios should be in the 'db' folder and the old files will be in the 'slicer-temp' folder\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NIJNjFTEav8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Optional) Unzip edited transcriptions\n",
        "#@markdown Unzip your own transcriptions into the `txt` folder so you don't need to use Whisper.\n",
        "\n",
        "file_location = '/content/drive/MyDrive/txt.zip' #@param {type:\"string\"}\n",
        "\n",
        "!7z x \"$file_location\" -o/content/txt\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Transcriptions extracted in txt folder\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mHmxkQ8qM_5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Whisper inference (Auto-transcriptions)**"
      ],
      "metadata": {
        "id": "gda9JsuGQ0ji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LGfA1ByTY_rI"
      },
      "outputs": [],
      "source": [
        "#@title Install Whisper\n",
        "!pip install -U openai-whisper\n",
        "!pip install ffmpeg\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"All done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Whisper inference\n",
        "#@markdown **Make transcriptions** <br/> Worth noting that your singing database shouldn't have long pauses, *ooh-ing*, lalala-ing, humming etc. in it, otherwise it'll probably break the transcription making (Whisper poorly recognises those).\n",
        "#Implemented from https://github.com/openai/whisper/discussions/1041 by Haru0l\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/txt/', exist_ok=True)\n",
        "!cd /content/db\n",
        "\n",
        "def Transcriber(audiofile):\n",
        "    import whisper\n",
        "    from whisper.tokenizer import get_tokenizer\n",
        "    #encourage model to transcribe words literally\n",
        "    tokenizer = get_tokenizer(multilingual=True)  # use multilingual=True if using multilingual model\n",
        "    number_tokens = [\n",
        "        i\n",
        "        for i in range(tokenizer.eot)\n",
        "        if all(c in \"0123456789\" for c in tokenizer.decode([i]).removeprefix(\" \"))\n",
        "    ]\n",
        "\n",
        "    model = whisper.load_model(\"medium\")\n",
        "    answer = model.transcribe(audiofile, suppress_tokens=[-1] + number_tokens)\n",
        "\n",
        "    print(answer['text'])\n",
        "\n",
        "    output_txt = os.path.join('/content/txt/', os.path.splitext(filename)[0] + '.txt')\n",
        "\n",
        "    with open(output_txt, 'w') as f:\n",
        "      f.write(answer['text'])\n",
        "\n",
        "for filename in os.listdir('/content/db/'):\n",
        "  if filename.endswith('.wav'):\n",
        "    file_path = os.path.join('/content/db/', filename)\n",
        "    Transcriber(file_path)\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Hopefully everything worked and your transcriptions are in the 'txt' folder!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9lYMRF3qa8Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Optional) Zip up text transcriptions `txt` for you to dowload and edit\n",
        "#@markdown Make sure there's only .txt files in your .zip archive!\n",
        "!zip -j transcriptions.zip /content/txt/*.txt\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Your transcriptions are now in 'transcriptions.zip'\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X5Aa8T-_eCHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Auto-alignment (MFA)**"
      ],
      "metadata": {
        "id": "crRwh8xgb8tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Condacolab\n",
        "#@markdown The session will crash and restart, that's normal!\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"All done, please wait for the session to restart!\")"
      ],
      "metadata": {
        "id": "TTxXkGZBmrs3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install MFA\n",
        "!conda install -c conda-forge montreal-forced-aligner spacy sudachipy sudachidict-core\n",
        "!pip install speechbrain\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"All done!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HY_iUJGBwT3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the alignment models\n",
        "#@markdown Choose the model for your desired language and scroll down to find the name of the model under \"Installation\"<br>After \"mfa model download acoustic/dictionary\" (e.g.: italian_cv)<br>Acoustic models: https://mfa-models.readthedocs.io/en/latest/acoustic/index.html<br>Dictionaries: https://mfa-models.readthedocs.io/en/latest/dictionary/index.html\n",
        "acoustic = 'italian_cv' #@param {type:\"string\"}\n",
        "dictionary = 'italian_cv' #@param {type:\"string\"}\n",
        "# Download Model\n",
        "!mfa model download acoustic \"$acoustic\"\n",
        "# Download G2P\n",
        "!mfa model download dictionary \"$dictionary\"\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Alignment models downloaded!\")\n"
      ],
      "metadata": {
        "id": "G5dox6_obfEp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start aligning!\n",
        "!mv /content/txt/*.txt /content/db\n",
        "!mfa align /content/db \"$dictionary\" \"$acoustic\" /content/alignment --beam 400\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"All done! Check the 'alignment' folder for .TextGrid files\")"
      ],
      "metadata": {
        "id": "6mU0FwKceLIL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LAB Converter**"
      ],
      "metadata": {
        "id": "5sC5QWZqcBEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install TextGrid to LAB converter\n",
        "!pip install mytextgrid\n",
        "!git clone https://github.com/gnloop/MFA-Universal-Notebook\n",
        "!mv /content/MFA-Universal-Notebook/text2lab_test.py /content/alignment/\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"All done!\")"
      ],
      "metadata": {
        "id": "TTbvrwN6HEsx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HALT! Before you go happily converting your TextGrid files, if your language is not present in the dropdown list below, you're gonna have to make your own 'custom_converter.txt' in the converters folder and choose it in the dropdown menu.\n",
        "#### Most MFA models use some sort of IPA system which doesn't sit well with DiffSinger.\n",
        "#### For further details on what phonemes MFA uses, you should check out the webpage where the MFA models are listed. There's usually a phoneme list there as well."
      ],
      "metadata": {
        "id": "4NPGauk2LiK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert TextGrid to LAB\n",
        "converter = 'converter_IT' #@param ['converter_JP', 'converter_EN-ARPA', 'converter_ES_tiny', 'converter_ES_Ryoku', 'converter_ES_njokis', 'converter_IT', 'custom_converter']\n",
        "!python -X utf8 /content/alignment/text2lab_test.py -c /content/MFA-Universal-Notebook/converters/\"$converter\".txt\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Your .lab files should now be under the 'alignment' folder!\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bi7CR7lNKTfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (ITALIAN ONLY) Fix labels\n",
        "#@markdown DO NOT USE FOR OTHER LANGUAGES<br>This will make its best attempts to fix some of the labels. However, manual editing will still be required.\n",
        "#@markdown <br><br>Known issues are:<br>- SH used instead of SK<br>- Poor estimation of consonant clusters<br>- Poor performance of the dictionary\n",
        "import re\n",
        "import os\n",
        "\n",
        "def process_lab_files(lab_file_path):\n",
        "    \"\"\"Divides double consonants, excluding 'rr', adjusts durations,\n",
        "       and replaces 'u' with 'w' and 'i' with 'y' if duration is less than 500000\n",
        "       and preceded by a consonant and followed by a vowel in the previous and next labels.\n",
        "\n",
        "    Args:\n",
        "        lab_file_path: The path to the .lab file.\n",
        "    \"\"\"\n",
        "    with open(lab_file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    modified_lines = []\n",
        "    for i, line in enumerate(lines):\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) == 3:  # Check if it's a valid label line\n",
        "            start_time, end_time, phoneme = parts\n",
        "\n",
        "            # Gemination fix: Divide double consonants\n",
        "            double_consonants = re.findall(r'(?!rr)([a-z])\\1', phoneme)\n",
        "            if double_consonants:\n",
        "                duration = float(end_time) - float(start_time)\n",
        "                half_duration = duration / 2\n",
        "                modified_lines.append(f\"{int(float(start_time))} {int(float(start_time) + half_duration)} {double_consonants[0]}\\n\")\n",
        "                modified_lines.append(f\"{int(float(start_time) + half_duration)} {int(float(end_time))} {double_consonants[0]}\\n\")\n",
        "                continue  # Skip to the next line to avoid processing the original line\n",
        "\n",
        "            # Cluster duration fix: Replace short 'u' and 'i' with exceptions\n",
        "            duration = int(end_time) - int(start_time)\n",
        "            if duration < 500000:\n",
        "                # Check for preceding consonant and following vowel in previous and next labels\n",
        "                prev_phoneme = lines[i - 1].strip().split()[-1] if i > 0 else \"\"\n",
        "                next_phoneme = lines[i + 1].strip().split()[-1] if i < len(lines) - 1 else \"\"\n",
        "\n",
        "                if re.search(r'[bcdfghjklmnprstvwxz]', prev_phoneme, re.IGNORECASE) and re.search(r'[aeiou]', next_phoneme, re.IGNORECASE):\n",
        "                    if phoneme == 'u':\n",
        "                        phoneme = 'w'\n",
        "                    elif phoneme == 'i':\n",
        "                        phoneme = 'y'\n",
        "\n",
        "            modified_lines.append(f\"{start_time} {end_time} {phoneme}\\n\")  # Append the modified or original line\n",
        "\n",
        "        else:\n",
        "            modified_lines.append(line)  # Keep lines that are not labels\n",
        "\n",
        "    with open(lab_file_path, 'w') as f:\n",
        "        f.writelines(modified_lines)\n",
        "\n",
        "# Specify the folder containing your .lab files\n",
        "folder_path = '/content/alignment/'  # Replace with your folder path\n",
        "\n",
        "# Loop through all files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.lab'):  # Process only .lab files\n",
        "        lab_file_path = os.path.join(folder_path, filename)\n",
        "        process_lab_files(lab_file_path)\n",
        "\n",
        "print(\"Fixed gemination and consonant clusters for all .lab files in the folder!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "seGx9djPG1n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip output\n",
        "# @markdown Enter the path where you want your labels saved (no / at the end)\n",
        "zip_path = '/content/drive/MyDrive/MFA_output' #@param {type:\"string\"}\n",
        "#@markdown Tick this box if you also wanna save the sliced samples.\n",
        "from IPython.display import display, HTML, Javascript\n",
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(zip_path, exist_ok=True)\n",
        "\n",
        "save_samples = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Zip wav files if save_samples is True\n",
        "if save_samples:\n",
        "    os.system(f\"zip -j {zip_path}/sliced_wavs.zip /content/db/*.wav\")\n",
        "\n",
        "# Zip labels\n",
        "!zip -j {zip_path}/labels.zip /content/alignment/*.lab\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"You can now download your labels in the labels.zip file and wavs in the sliced_wavs.zip file!\")"
      ],
      "metadata": {
        "id": "uRn0oWwkM5ny",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Extras**"
      ],
      "metadata": {
        "id": "6n0MUMW6OZkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Whisper (Transformers) Install\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade transformers accelerate torchaudio\n",
        "!pip install \"punctuators==0.0.5\"\n",
        "!pip install \"pyannote.audio\"\n",
        "!pip install git+https://github.com/huggingface/diarizers.git"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XSwNWrabkMqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Whisper (Transformers) Inference\n",
        "#@markdown **Make transcriptions a bit faster with community-made models** <br/> Whisper (Transformers) is the implementation of Whisper by HuggingFace, 7x times faster than the common Whisper, it is placed in extra options due to its complexity of use; it allows the use of models made by the community, perfect for complex accent audios that would require a fine-tuned model.\n",
        "#Implemented from https://github.com/openai/whisper/discussions/1041 by Haru0l, edited to be compatible with Transformers HuggingFace implementation\n",
        "\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "os.makedirs('/content/txt/', exist_ok=True)\n",
        "\n",
        "#@markdown <b><font size=\"3.5\"> Model name (HuggingFace)\n",
        "model_name = \"openai/whisper-medium\"  #@param {type:\"string\"}\n",
        "#@markdown <b><font size=\"3.5\"> generate_kwarts\n",
        "language = \"Japanese\"  #@param {type:\"string\"}\n",
        "no_repeat_ngram_size = 0  #@param {type:\"integer\"}\n",
        "repetition_penalty = 1.0  #@param {type:\"number\"}\n",
        "\n",
        "generate_kwargs = {\n",
        "    \"language\": language,\n",
        "    \"no_repeat_ngram_size\": no_repeat_ngram_size,\n",
        "    \"repetition_penalty\": repetition_penalty,\n",
        "}\n",
        "\n",
        "def transcriber(audiofile):\n",
        "    transcriber = pipeline(\"automatic-speech-recognition\", model=model_name, generate_kwargs=generate_kwargs)\n",
        "\n",
        "    print(f\"Processing files: {audiofile}\")\n",
        "    result = transcriber(audiofile)\n",
        "\n",
        "    transcription = result['text']\n",
        "    print(transcription)\n",
        "\n",
        "    output_txt = os.path.join('/content/txt/', os.path.splitext(os.path.basename(audiofile))[0] + '.txt')\n",
        "    with open(output_txt, 'w') as f:\n",
        "        f.write(transcription)\n",
        "\n",
        "audio_directory = '/content/db/'\n",
        "for filename in os.listdir(audio_directory):\n",
        "    if filename.endswith('.wav'):\n",
        "        file_path = os.path.join(audio_directory, filename)\n",
        "        transcriber(file_path)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Hopefully everything worked and your transcriptions are in the 'txt' folder!\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1L-Sx48oOh8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title WhisperX Install\n",
        "!pip install ffmpeg\n",
        "!pip install git+https://github.com/m-bain/whisperx.git"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4WzO4_cXkGdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title WhisperX inference\n",
        "# @markdown **Make transcriptions faster with optional forced-alignment** <br/> WhisperX is an alternative 70x times faster than the common Whisper, it is placed in extra options due to its complexity of use; it allows time stamping for words and/or sentences, which can be useful to use it as a slicer.\n",
        "\n",
        "import os\n",
        "import whisperx\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# @markdown ### Select Model, Language, and Batch Size\n",
        "Model = \"large-v3\"  # @param [\"tiny\", \"tiny.en\", \"base\", \"base.en\", \"small\", \"small.en\", \"medium\", \"medium.en\", \"large-v1\", \"large-v2\", \"large-v3\"]\n",
        "Language = \"ja\"  # @param {type:\"string\"}\n",
        "Batch_size = 9  # @param {type:\"integer\"}\n",
        "# @markdown <font size=\"-1.5\">\n",
        "Chunk_size = 30 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Forced Alignment Options\n",
        "Forced_alignment = False  # @param {type:\"boolean\"}\n",
        "Alignment_mode = \"Sentence-level\" # @param [\"Word-level\", \"Sentence-level\"] {allow-input: false}\n",
        "Export_format = \"Audacity\"  # @param [\"Audacity\", \".lab\"] {allow-input: false}\n",
        "\n",
        "# @markdown <font size=\"-1.5\"> the maximum number of words in a line before breaking the line (default: None)\n",
        "Max_words_per_line = None # @param {type:\"integer\"}\n",
        "# @markdown <font size=\"-1.5\"> the maximum number of characters in a line before breaking the line (default: None)\n",
        "Max_line_width = None # @param {type:\"integer\"}\n",
        "# @markdown <font size=\"-1.5\"> the maximum number of lines in a segment (default: None)\n",
        "Max_line_count = None # @param {type:\"integer\"}\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('/content/forced-alignment/', exist_ok=True)\n",
        "os.makedirs('/content/txt/', exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 1. Transcribe with original whisper (batched)\n",
        "model = whisperx.load_model(Model, device, compute_type=\"float16\", language=Language)\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "    result = model.transcribe(audio, batch_size=Batch_size, language=Language, chunk_size=Chunk_size)\n",
        "    return result\n",
        "\n",
        "def align_audio(result, audio_file):\n",
        "    # 2. Align the text to the audio\n",
        "    model_a, metadata = whisperx.load_align_model(language_code=Language, device=device)\n",
        "    result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, audio_file, device, return_char_alignments=False)\n",
        "    return result_aligned\n",
        "\n",
        "def format_transcription(result, mode, max_words=None, max_width=None, max_lines=None):\n",
        "    \"\"\"Formats the transcription according to the specified parameters.\"\"\"\n",
        "    formatted_lines = []\n",
        "    if mode == \"Word-level\":\n",
        "        for segment in result[\"segments\"]:\n",
        "            for word in segment[\"words\"]:\n",
        "                if word[\"start\"] is not None and word[\"end\"] is not None:\n",
        "                    formatted_lines.append(f\"{word['start']:.3f}\\t{word['end']:.3f}\\t{word['word']}\")\n",
        "\n",
        "    elif mode == \"Sentence-level\":\n",
        "        for segment in result[\"segments\"]:\n",
        "            if segment[\"start\"] is not None and segment[\"end\"] is not None:\n",
        "                current_line = []\n",
        "                current_line_width = 0\n",
        "                line_count = 0\n",
        "                segment_start_time = segment['start']\n",
        "                segment_end_time = segment['end']\n",
        "                words_with_times = [(word, w['start'], w['end']) for word, w in zip(segment['text'].strip().split(), segment['words'])]\n",
        "\n",
        "                for i, (word, start, end) in enumerate(words_with_times):\n",
        "                    if (max_words and len(current_line) >= max_words) or \\\n",
        "                       (max_width and current_line_width + len(word) + 1 > max_width) or \\\n",
        "                       (max_lines and line_count >= max_lines):\n",
        "\n",
        "                        # Determine end time of the current line\n",
        "                        line_end_time = words_with_times[i - 1][2] if i > 0 else segment_start_time\n",
        "\n",
        "                        formatted_lines.append(f\"{segment_start_time:.3f}\\t{line_end_time:.3f}\\t\" + \" \".join(current_line))\n",
        "                        current_line = []\n",
        "                        current_line_width = 0\n",
        "                        line_count += 1\n",
        "                        segment_start_time = line_end_time  # Update start time for the next line\n",
        "\n",
        "                    current_line.append(word)\n",
        "                    current_line_width += len(word) + 1\n",
        "\n",
        "                # Handle the last line in the segment\n",
        "                if current_line:\n",
        "                    formatted_lines.append(f\"{segment_start_time:.3f}\\t{segment_end_time:.3f}\\t\" + \" \".join(current_line))\n",
        "\n",
        "    return formatted_lines\n",
        "\n",
        "def export_labels(result_aligned, filename):\n",
        "    output_filename = os.path.join('/content/forced-alignment/', os.path.splitext(filename)[0])\n",
        "\n",
        "    formatted_transcription = format_transcription(\n",
        "        result_aligned,\n",
        "        Alignment_mode,\n",
        "        Max_words_per_line,\n",
        "        Max_line_width,\n",
        "        Max_line_count\n",
        "    )\n",
        "\n",
        "    extension = \".txt\" if Export_format == \"Audacity\" else \".lab\"\n",
        "    with open(output_filename + extension, \"w\") as f:\n",
        "        for line in formatted_transcription:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "for filename in os.listdir('/content/db/'):\n",
        "    if filename.endswith('.wav'):\n",
        "        file_path = os.path.join('/content/db/', filename)\n",
        "\n",
        "        result = transcribe_audio(file_path)\n",
        "        full_text = \" \".join([segment[\"text\"] for segment in result[\"segments\"]])\n",
        "\n",
        "        output_txt = os.path.join('/content/txt/', os.path.splitext(filename)[0] + '.txt')\n",
        "        with open(output_txt, 'w') as f:\n",
        "            f.write(full_text)\n",
        "\n",
        "        if Forced_alignment:\n",
        "            result_aligned = align_audio(result, file_path)\n",
        "            export_labels(result_aligned, filename)\n",
        "\n",
        "clear_output()\n",
        "print(\"Transcription complete! Transcriptions are in the 'txt' folder.\")\n",
        "if Forced_alignment:\n",
        "    print(f\"Forced alignment labels ({Alignment_mode}) are in the 'forced-alignment' folder.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TnWSCA16hkQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}